# Emotion Recognition Using Speech Processing

This project focuses on recognizing human emotions from speech using advanced machine learning and deep learning techniques. The system extracts meaningful features from audio files and classifies them into emotions such as happiness, sadness, anger, and more. It also provides a user-friendly web interface for real-time emotion detection.

---


## Overview
### Problem Statement
Existing emotion recognition systems often struggle with accuracy and scalability, especially in real-time applications. This project aims to bridge this gap by leveraging deep learning models and advanced feature extraction techniques.

### Proposed Solution
Our system uses a Convolutional Neural Network (CNN) to classify emotions based on audio features like Mel Frequency Cepstral Coefficients (MFCC). It combines multiple datasets to ensure diversity and improved performance.

---

## Features
- Classify emotions like happy, sad, angry, neutral, etc.
- Web-based interface for real-time predictions.
- Scalable and efficient model with higher accuracy.

---

## Technologies Used
- **Programming Language**: Python  
- **Frontend**: HTML, CSS  
- **Backend**: Flask  
- **Machine Learning**: TensorFlow, scikit-learn  
- **Audio Processing**: librosa  
- **Visualization**: Matplotlib, Seaborn  

---

## Datasets
The project uses multiple publicly available datasets to ensure diversity and accuracy:
- **RAVDESS**: Ryerson Audio-Visual Database of Emotional Speech and Song  
- **CREMA-D**: Crowd-sourced Emotional Multimodal Actors Dataset  
- **TESS**: Toronto Emotional Speech Set  
- **SAVEE**: Surrey Audio-Visual Expressed Emotion  

---

